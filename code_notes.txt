Code notes:
opensfm_run_all
	extract_metadata
	detect_features
	match_features
	create_tracks
	reconstruct
	mesh

opensfm
	commands
		extract_metadata
			image width, length
			make and model
			GPano:ProjectionType 
			Focal length
			Orientation
			Allows for camera model override
		detect_features
			find mask for images
			extract_features(image, config, mask)
				extract_features_sift
					config
						edge_threshold
						peak_threshold
					cv2.FeatureDetector
					cv2.DescriptorExtractor
					reduce peak threshold by 2/3 in a loop until enough pts
					feature_min_frames determines when enough pts
				remove feature points outside of mask
				normalize points
					center of image is (0, 0)
					coordinates between -0.5 and 0.5
				points: [(x, y, size, angle)]
				features: [(length 128 descriptor)]
				colors: [color at point]
				return points, features, colors
			preemptive_features = top "preemptive_max" points sorted by size of feature
			save features / preemptive features
			build_flann_index / save flann index - fast library for approximate nearest neighbors
		match_features
			match_candidates_from_metadata
				invent_reference_lla - creates longitude / latitude reference from exifs of images
				set gps neighbors to 0 if not all exifs exist
				if no pair selection strats, match all combinations
				else
					match_candidates_by_distance
						return nothing if both max_neighbors and max_distance are 0
						convert long lat to xyz around a reference
						put coords into a kd tree
						for every image, find the max_neighbors number of other images closest to it
					match_candidates_by_time
						return nothing if max_neighbors = 0
						find the max_neighbors number of other images closest in capture time
					match_candidates_by_order
						return nothing if max_neighbors = 0
						find the max_neighbors number of images in order of name around image
				return {image -> [images to match]}
			Context {
				data : dataset,
				cameras : dataset.load_camera_models(),
				exifs : {im -> dataset.load_exif(im)},
				p_pre, f_pre : dataset.load_preemptive_features()
			}
			match
				match a image im1 with all of its match candidates
				for each candidate
					if preemptive_threshold > 0: preemptive match with preemptive_lowes_ratio
						match_lowe_bf - brute force match
						if number of matches is less than preemptive_threshold, skip this candidate
					load flann index
					matching.match_symmetric to get feature point matches
						if number of matches is less than robust_matching_min_match, skip this candidate
					matching.robust_match to match with cameras
						if number of matches is less than robust_matching_min_match, skip this candidate
				save match for im1 {
					im2 : [(feature index in im1, feature index in im2), ...]
				}
		create_tracks
			features: {im -> [(x, y) of points]}
			colors: {im -> color}
			matches: {
				(im1, im2) : [(feature index in im1, feature index in im2), ...]
			}
			matching.create_tracks_graph
			save tracks graph
		reconstruct
			reconstruction.incremental_reconstruction

	dataset: 
		config
		image list
		mask list
		save and load methods
		{
			data_path : path to dir containing dataset,
			config / _load_config() : { configs },
			image_list / images() : [names of image files],
			mask_list / masks() : [names of mask files],
			mask_files : {name -> path},
			load_exif(image) : {
				# common fields:
				width, height, # of image
				focal_prior, # focal length (real) / sensor width
				gps : {
					latitude,
					longitude
				}
			},
			load_camera_models() : [{
				# for perspective
				projection_type,
				width, height, # image dimensions
				focal, focal_prior,
				k1, k2,
				k1_prior, k2_prior
			}],
			load_features(image) : (points, descriptors, colors),
			load_preemptive_features(image) : (points, descriptors),
			load_feature_index(image, features) : FLANN index of the descriptors,
			invent_reference_lla : {
				# weighted from all image exifs that have gps
				lat, lon, alt
			}
		}

	features
		build_flann_index
			method = KMEANS if features is float, else LSH

	matching
		match_lowe_bf - brute force matching - https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_matcher/py_matcher.html
			create bruteforce matcher
			find the best / secondbest match
			lowes ratio test: if best / second best > ratio, reject match
		match_lowe
			takes in flann index of image 1 and array of feature vectors of image 2
			look for best two matches for image 2 features in image 1 index
			keep the matches that pass lowes ratio test
		match_symmetric
			match points in image 2 to image 1
			match points in image 1 to image 2
			take the intersection
		robust_match_fundamental
			use ransac in cv2.findFundamentalMat
			return inliers
		robust_match_calibrated
			calculate bearing vectors for pixels
				bearing vector = unit vector pointing to pixel viewing direction
			relative pose ransac - Stewenius
			return inliers
		robust_match
			if camera is calibrated already - camera parameters are perspective and k's are 0
				robust_match_fundamental
			else
				robust_match_calibrated
		create_tracks_graph
			group together (im1, feature_id_1) and (im2, f_id_2) with union find
				clusters are tracks - points in space
			sets: {
				cluster: [(im, f_id), ...]
			}
			keep only tracks longer than min_track_length and that don't contain two of the same points from the same image
			tracks: [
				[(im, f_id), ...],
				track2,
				track3,
				...
			]
			create bipartite graph between images and tracks
				add edge between image and track for every feature, with feature = (x, y) coordinate in the image, featureid, and feature color (r, g, b)
			tracks_graph (bipartite):
				image node
				track_id node
				edge between image and track_id {
					feature : (x, y),
					feature_id,
					feature_color
				}
		all_common_tracks
			track_dict: {
				# all combinations of (im1, im2) that share a track
				(im1, im2) : [tracks in common],
			}
			common_tracks : { # makes sure each image pair has at least 50 tracks in common
				(im1, im2) : (tracks in common, [points for tracks in im1], [points for tracks in im2]) # if include_features
				(im1, im2) : tracks in common # if not include_features
			}
			return common_tracks
	reconstruction
		incremental_reconstruction
			tracks: list of all track ids
			images: list of all image names
			load ground control points if present
			get common_tracks / points between every two images
			pairs = compute_image_pairs
			reconstructions = list of partial reconstructions
			iterate through pairs of (im1, im2) in order of highest reconstructability
				if im1 or im2 have be used for reconstruction before, skip
				reconstruction = bootstrap_reconstruction(...)
				grow_reconstruction() with remaining images
				save reconstruction
		compute_image_pairs
			for every pair of images (im1, im2) in track_dict, run _compute_pair_reconstructability on lists of correspondence points p1 and p2
			result = [(im1, im2, r) ...]
			keep the pairs with r > 0, return them in order of reconstructability
		_compute_pair_reconstructability
			cv2.findHomography(p1, p2, ...)
				finds perspective transformation of the points from im1 coordinates to im2 coordinates
				returns transformation, inlier indices
			r = pairwise_reconstructability(...)
			returns (im1, im2, r)
		pairwise_reconstructability
			return r = 0 if number of fraction of outliers > 0.3, else returns number of tracks
		bootstrap_reconstruction
			use 5 point algo
			R, t, inliers = two_view_reconstruction(...)
			if less than 5 inliers, skip
			construct reconstruction data
			triangulate_shot_features adds tracks as points into the reconstruction
			bundle_single_view adjusts for im2
			retriangulate after bundle adjustment
			bundle_single_view adjusts for im2 again
			return reconstruction
		two_view_reconstruction
			http://laurentkneip.github.io/opengv/page_how_to_use.html
			run relative pose ransac with pyopengv, Stewenius method
			_two_view_reconstruction_inliers used to find the inlier indices
			run relative pose ransac with pyopengv again, only with the inliers
			recompute the inliers
			return transformation matrices and inliers
		_two_view_reconstruction_inliers

		triangulate_shot_features
			use TrackTriangulator
			for all tracks that im1 is part of
				if track isn't in reconstruction: TrackTriangulator.triangulate
		TrackTriangulator
			triangulate
				os : [coordinates of image origins in reconstruction space]
				bs : [orientations of bearings in reconstruction space]
				iterate over shots that contain a track
					if shot is in reconstruction already
						add shot origin to os, shot bearings to bs
				e, X = csfm.triangulate_bearings_midpoint
		bundle_single_view
			bundle adjust a single camera
		retriangulate
			iterate through all the points and triangulate them with the camera positions in the reconstruction
		grow_reconstruction
			bundle
			align.align_reconstruction to align with ground control points and gps data
			loop:
				reconstructed_points_for_images
					return [(im, number of points in reconstruction), ...] for im not in reconstruction, in order of dec # of points
				loop over image, num_tracks:
					if resect image
						triangulate image with reconstruction
						bundle as necessary - bundle, remove outliers, align reconstructions
						retriangulate as necessary
			bundle
			align.align_reconstruction
			paint_reconstruction
		bundle
			bundle adjusts the reconstruction

	types
		Reconstruction { # reconstructed scene
			cameras : {camera_id : camera},
			shots : {shot_id : shot}, # shot_id is the image name
			points : {point_id : point}
		}
		Shot {
			id : image name,
			camera : image camera,
			pose : pose of the camera in reconstruction (camera 1) coordinates,
			metadata : shot metadata
		}
		Pose {
			rotation : 3x1 vector,
			translation : 3x1 vector
		}
		ShotMetadata {
			gps_position : (x, y, z) relative to reference,
			gps_dop : dilute of precision - larger is less precise,
			orientation,
			accelerometer,
			compass,
			capture_time,
			skey
		}
		Point {
			id : track_id,
			coordinates : list containing the 3D positions
		}
	csfm
		triangulate_bearings_midpoint
	align
		align_reconstruction

sift
	extrema detection
		filter with laplacian of gaussian (approx diff of gaussian)
		find local extremum across different scale (different sigma)
		search for local extremum across scale and space
	keypoint localization
		refine extrema location
		reject low intensity extrema "contrastThreshold"
		discard the edges: compute principle curvature - similar to harris - if ratio over "edgeThreshold", discard
	orientation assignment
		take a neighborhood around keypoint
		build orientation histogram with 36 bins covering 360 degrees 
		weigh score in neighborhood according to magnitude and gaussian of distance to the keypoint
		take highest peak from histogram and any peak 80% of the highest peak
	descriptor calculation
		16x16 neighborhood around keypoint
			16 subblocks of 4x4
			8 orientation bins per subblock
			128 bins total -> vector of 128
				adjust for illumination changes, other factors
	matching
		match keypoints by identifying closest neighbor in feature space
		if ratio of closest match distance to second closest match distance is > 0.8, then both are rejected
			eliminates 90% false matches while losing 5% true matches

