{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "\n",
    "import sys, os, itertools, pickle\n",
    "from pprint import pprint\n",
    "\n",
    "from util import *\n",
    "from config import *\n",
    "from putil import *\n",
    "%load_ext autoreload\n",
    "%aimport util\n",
    "%aimport putil\n",
    "%aimport config\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_k(it, k):\n",
    "    return [next(it) for _ in range(k)][-1]\n",
    "\n",
    "def show_img(imgs, figsize=(12, 9)):\n",
    "    if type(imgs) != list:\n",
    "        imgs = [imgs]\n",
    "    plt.figure(figsize=figsize)\n",
    "    alpha = 1\n",
    "    alpha_dec = 1 / len(imgs)\n",
    "    for img in imgs:\n",
    "        plt.imshow(img, alpha=alpha)\n",
    "        alpha -= alpha_dec\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def to_homo(x):\n",
    "    for axis, size in enumerate(x.shape):\n",
    "        if size == 2:\n",
    "            break\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "    ones_shape = list(x.shape)\n",
    "    ones_shape[axis] = 1\n",
    "    return np.concatenate((x, np.ones(ones_shape)), axis=axis)\n",
    "\n",
    "def mult_homo(M, x):\n",
    "    n, d = x.shape\n",
    "    if d == 2:\n",
    "        x = to_homo(x)\n",
    "    output = np.dot(x, M.T)\n",
    "    return output[:, :2] / output[:, 2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_frames_path = Root + 'cached_frames.p'\n",
    "if os.path.exists(cached_frames_path):\n",
    "    frames = load_pickle(cached_frames_path)\n",
    "else:\n",
    "    frames = {}\n",
    "    for serie in 'pool_room', 'cory_breezeway', 'soda_front':\n",
    "        calibrated_dir = Data + serie + '/calibrated/'\n",
    "        videos = [read_video(get_data_path(calibrated_dir, i, name_is_dir=True)) for i in Cam_ids]\n",
    "        k = 200\n",
    "        for cam, video in zip(Cam_ids, videos):\n",
    "            for i, frame in enumerate(video):\n",
    "                if i % k == 0:\n",
    "                    frames.setdefault((serie, i), {})[cam] = np.array(frame)\n",
    "        for video in videos:\n",
    "            video.close()\n",
    "    save_pickle(frames, cached_frames_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = { i : np.load(os.path.join(Calibrations, str(i), 'new_camera_matrix.npy')) for i in Cam_ids }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# { (camera1, camera2) : ((y_min, y_max, x_min, x_max)1, (y_min, y_max, x_min, x_max)2) }\n",
    "masks = {}\n",
    "masks[(1, 2)] = ((0, Video_height, 2000, Video_width), (0, 600, 300, 2000))\n",
    "masks[(1, 3)] = ((0, Video_height, 0, 700), (1500, Video_height, 0, 2100))\n",
    "masks[(1, 4)] = ((0, 600, 300, 2000), (0, Video_height, 2150, Video_width))\n",
    "masks[(1, 5)] = ((1400, Video_height, 600, 2100), (0, Video_height, 2000, Video_width))\n",
    "masks[(2, 4)] = ((0, Video_height, 2100, Video_width), (0, 400, 650, 2100))\n",
    "masks[(2, 5)] = ((0, Video_height, 0, 650), (1400, Video_height, 600, 2100))\n",
    "masks[(2, 6)] = ((1600, Video_height, 800, 2350), (0, Video_height, 2150, Video_width))\n",
    "masks[(3, 4)] = ((0, Video_height, 2000, Video_width), (1500, Video_height, 500, 2200))\n",
    "masks[(3, 5)] = ((0, Video_height, 0, 500), (0, 400, 650, 2100))\n",
    "masks[(3, 6)] = ((0, 500, 500, 2200), (0, Video_height, 0, 600))\n",
    "masks[(4, 6)] = ((0, Video_height, 0, 700), (1400, Video_height, 300, 2100))\n",
    "masks[(5, 6)] = ((0, Video_height, 0, 700), (0, 400, 600, 2250))\n",
    "\n",
    "def get_mask(y_min, y_max, x_min, x_max):\n",
    "    mask = np.zeros((Video_height, Video_width), dtype=np.uint8)\n",
    "    mask[y_min : y_max, x_min : x_max] = 1\n",
    "    return mask\n",
    "\n",
    "def in_mask(kp, y_min, y_max, x_min, x_max):\n",
    "    x, y = kp.pt\n",
    "    return (y_min <= y < y_max) and (x_min <= x < x_max)\n",
    "\n",
    "full_masks = {} # { camera : overlap of all masks }\n",
    "for c in Cam_ids:\n",
    "    mask = None\n",
    "    for (c1, c2), (cors1, cors2) in masks.items():\n",
    "        if c1 == c:\n",
    "            cors = cors1\n",
    "        elif c2 == c:\n",
    "            cors = cors2\n",
    "        else:\n",
    "            continue\n",
    "        new_mask = get_mask(*cors)\n",
    "        if mask is None:\n",
    "            mask = new_mask\n",
    "        else:\n",
    "            mask |= new_mask\n",
    "    full_masks[c] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for (c1, c2), (corners1, corners2) in sorted(masks.items()):\n",
    "    show_img([frames['pool_room', 0][c1], get_mask(*corners1)], figsize=(8, 6))\n",
    "    show_img([frames['pool_room', 0][c2], get_mask(*corners2)], figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match_indices(fs, keypoints, masks, c1, c2, debug=False):\n",
    "    cors1, cors2 = masks[c1, c2]\n",
    "    def get_keypoint_indices(c, cors):\n",
    "        kps, des = keypoints[c] # all keypoints for a given camera\n",
    "        # unique ids of points in the mask\n",
    "        kp_ids = [i for i, p in enumerate(kps) if in_mask(p, *cors)]\n",
    "        # unique points in the mask\n",
    "        kps_masked = [kps[i] for i in kp_ids]\n",
    "        des_masked = np.array([des[i] for i in kp_ids])\n",
    "        return kps_masked, des_masked, kp_ids\n",
    "    kp1, des1, kp_ids1 = get_keypoint_indices(c1, cors1)\n",
    "    kp2, des2, kp_ids2 = get_keypoint_indices(c2, cors2)\n",
    "#     show_img([fs[c1], get_mask(*cors1)])\n",
    "#     show_img([fs[c2], get_mask(*cors2)])\n",
    "\n",
    "    k = 2\n",
    "    if len(des1) < k or len(des2) < k:\n",
    "        return None\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good = [m for m, n in matches if m.distance < 0.6 * n.distance]\n",
    "    match_indices = [(kp_ids1[m.queryIdx], kp_ids2[m.trainIdx]) for m in good]\n",
    "\n",
    "    if debug:\n",
    "        if len(good) >= 8:\n",
    "            pts1, pts2 = map(np.array, zip(*[(kp1[m.queryIdx].pt, kp2[m.trainIdx].pt) for m in good]))\n",
    "\n",
    "            H, mask = cv2.findHomography(pts1, pts2, method=cv2.RANSAC, ransacReprojThreshold=1)\n",
    "            mask = mask.ravel().tolist()\n",
    "            side_by_side = cv2.drawMatches(\n",
    "                np.array(fs[c1]), kp1,\n",
    "                np.array(fs[c2]), kp2,\n",
    "                good,\n",
    "                None,\n",
    "                matchesMask=mask,\n",
    "                matchColor=(0, 255, 0),\n",
    "                singlePointColor=(255, 0, 0),\n",
    "                flags=cv2.DRAW_MATCHES_FLAGS_DEFAULT\n",
    "            )\n",
    "            show_img(side_by_side, figsize=(24, 9))\n",
    "        else:\n",
    "            print('Not enough good points: only %s' % len(good))\n",
    "    if len(match_indices) < 8:\n",
    "        return None\n",
    "    return match_indices\n",
    "\n",
    "def compute_keypoints(frames, masks, full_masks):\n",
    "    keypoints = {} # { camera : (keypoints, descriptors)}\n",
    "    for im, mask in full_masks.items():\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        kp, des = sift.detectAndCompute(frames[im], mask)\n",
    "        keypoints[im] = (kp, des)\n",
    "    \n",
    "    correspondences = {}\n",
    "    for c1, c2 in masks.keys():\n",
    "        match_indices = find_match_indices(fs, keypoints, masks, c1, c2)\n",
    "        if match_indices:\n",
    "            correspondences[c1, c2] = match_indices\n",
    "#         print('%s correspondences between images %s and %s' % (len(match_indices), c1, c2))\n",
    "    return keypoints, correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "for label, fs in random.sample(frames.items(), 5):\n",
    "    # all keypoints within a frame\n",
    "    keypoints = {} # { camera : (keypoints, descriptors) }\n",
    "    for im, mask in full_masks.items():\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        kp, des = sift.detectAndCompute(fs[im], mask)\n",
    "        keypoints[im] = (kp, des)\n",
    "        \n",
    "    for c1, c2 in random.sample(masks.keys(), 3):\n",
    "        print(label, c1, c2)\n",
    "        find_match_indices(fs, keypoints, masks, c1, c2, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_maps = { c : {} for c in Cam_ids }\n",
    "merged_keypoints = { c : [] for c in Cam_ids }\n",
    "merged_correspondences = {}\n",
    "for label, fs in frames.items():\n",
    "    print(label)\n",
    "    keypoints, correspondences = compute_keypoints(fs, masks, full_masks)\n",
    "    for pair, match_indices in correspondences.items():\n",
    "        indices_list = zip(*match_indices)\n",
    "        for c, indices in zip(pair, indices_list):\n",
    "            index_map = index_maps[c]\n",
    "            kps = keypoints[c][0]\n",
    "            for index in indices:\n",
    "                if index not in index_map:\n",
    "                    index_map[index] = len(index_map)\n",
    "                    merged_keypoints[c].append(kps[index])\n",
    "        c1, c2 = pair\n",
    "        for i1, i2 in match_indices:\n",
    "            merged_correspondences.setdefault(pair, []).append((index_maps[c1][i1], index_maps[c2][i2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(Root + 'OpenSfM/')\n",
    "from opensfm import matching\n",
    "features = { cam : mult_homo(np.linalg.inv(cameras[cam]), np.array([x.pt for x in kps])) for cam, kps in merged_keypoints.items() }\n",
    "colors = { cam : [(0, 0, 0) for _ in kps] for cam, kps in merged_keypoints.items() }\n",
    "run_config = { 'min_track_length' : 2, 'processes' : 5 }\n",
    "tracks_graph = matching.create_tracks_graph(features, colors, merged_correspondences, run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     7
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'not_reconstructed_images': [],\n",
       " 'num_candidate_image_pairs': 12,\n",
       " 'reconstructions': [{'bootstrap': {'common_tracks': 469,\n",
       "    'decision': 'Success',\n",
       "    'image_pair': (1, 2),\n",
       "    'memory_usage': 4456210432,\n",
       "    'triangulated_points': 391,\n",
       "    'two_view_reconstruction': {'5_point_inliers': 391,\n",
       "     'method': '5_point',\n",
       "     'plane_based_inliers': 388}},\n",
       "   'grow': {'steps': []}},\n",
       "  {'bootstrap': {'common_tracks': 272,\n",
       "    'decision': 'Success',\n",
       "    'image_pair': (3, 4),\n",
       "    'memory_usage': 4456210432,\n",
       "    'triangulated_points': 248,\n",
       "    'two_view_reconstruction': {'5_point_inliers': 246,\n",
       "     'method': '5_point',\n",
       "     'plane_based_inliers': 165}},\n",
       "   'grow': {'steps': []}},\n",
       "  {'bootstrap': {'common_tracks': 133,\n",
       "    'decision': 'Success',\n",
       "    'image_pair': (5, 6),\n",
       "    'memory_usage': 4456210432,\n",
       "    'triangulated_points': 111,\n",
       "    'two_view_reconstruction': {'5_point_inliers': 111,\n",
       "     'method': 'plane_based',\n",
       "     'plane_based_inliers': 111}},\n",
       "   'grow': {'steps': []}}],\n",
       " 'wall_times': {'compute_image_pairs': 8.274284839630127,\n",
       "  'compute_reconstructions': 1.9052109718322754,\n",
       "  'load_tracks_graph': 0.0036280155181884766}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opensfm.reconstruction import *\n",
    "from opensfm import dataset\n",
    "from opensfm import types\n",
    "import opensfm.config\n",
    "import yaml\n",
    "reload(reconstruction)\n",
    "\n",
    "class DataMock(dataset.DataSet):\n",
    "    def __init__(self):\n",
    "        self.reference_lla = None\n",
    "        self.config = yaml.load(opensfm.config.default_config_yaml)\n",
    "        self.config.update(run_config)\n",
    "    \n",
    "    def reference_lla_exists(self):\n",
    "        return self.reference_lla is not None\n",
    "    \n",
    "    def save_reference_lla(self, ref):\n",
    "        self.reference_lla = ref\n",
    "\n",
    "    def load_reference_lla(self):\n",
    "        return self.reference_lla\n",
    "        \n",
    "    def images(self):\n",
    "        return list(range(1, Num_cameras + 1))\n",
    "    \n",
    "    def load_exif(self, cam_index):\n",
    "        cam_mat = cameras[cam_index]\n",
    "        return {\n",
    "            'width' : Video_width / cam_mat[0, 0],\n",
    "            'height' : Video_height / cam_mat[1, 1],\n",
    "            'focal_prior' : 1,\n",
    "            'camera' : cam_index\n",
    "        }\n",
    "    \n",
    "    def load_tracks_graph(self):\n",
    "        return tracks_graph\n",
    "    \n",
    "    def ground_control_points_exist(self):\n",
    "        return False\n",
    "    \n",
    "    def load_camera_models(self):\n",
    "        def create_camera(i):\n",
    "            cam_mat = cameras[i]\n",
    "            camera = types.PerspectiveCamera()\n",
    "            camera.id = i\n",
    "            camera.width = Video_width / cam_mat[0, 0]\n",
    "            camera.height = Video_height / cam_mat[1, 1]\n",
    "            camera.focal = camera.focal_prior = 1\n",
    "            camera.k1 = camera.k1_prior = 0\n",
    "            camera.k2 = camera.k2_prior = 0\n",
    "            return camera\n",
    "        return { i : create_camera(i) for i, camera in cameras.items() }\n",
    "    \n",
    "    def save_reconstruction(self, reconstructions, filename=None, minify=False):\n",
    "        self.reconstructions = reconstructions\n",
    "    \n",
    "    def load_reconstruction(self, filename=None):\n",
    "        return self.reconstructions\n",
    "    \n",
    "data = DataMock()\n",
    "\n",
    "def incremental_reconstruction(data):\n",
    "    \"\"\"Run the entire incremental reconstruction pipeline.\"\"\"\n",
    "    logger.info(\"Starting incremental reconstruction\")\n",
    "    report = {}\n",
    "    chrono = Chronometer()\n",
    "    if not data.reference_lla_exists():\n",
    "        data.invent_reference_lla()\n",
    "\n",
    "    graph = data.load_tracks_graph()\n",
    "    tracks, images = matching.tracks_and_images(graph)\n",
    "    chrono.lap('load_tracks_graph')\n",
    "    remaining_images = set(images)\n",
    "    gcp = None\n",
    "    if data.ground_control_points_exist():\n",
    "        gcp = data.load_ground_control_points()\n",
    "    common_tracks = matching.all_common_tracks(graph, tracks)\n",
    "    reconstructions = []\n",
    "    pairs = compute_image_pairs(common_tracks, data)\n",
    "    chrono.lap('compute_image_pairs')\n",
    "    report['num_candidate_image_pairs'] = len(pairs)\n",
    "    report['reconstructions'] = []\n",
    "    for im1, im2 in pairs:\n",
    "        if im1 in remaining_images and im2 in remaining_images:\n",
    "            rec_report = {}\n",
    "            report['reconstructions'].append(rec_report)\n",
    "            tracks, p1, p2 = common_tracks[im1, im2]\n",
    "            reconstruction, rec_report['bootstrap'] = bootstrap_reconstruction(\n",
    "                data, graph, im1, im2, p1, p2)\n",
    "\n",
    "            if reconstruction:\n",
    "                remaining_images.remove(im1)\n",
    "                remaining_images.remove(im2)\n",
    "                reconstruction, rec_report['grow'] = grow_reconstruction(\n",
    "                    data, graph, reconstruction, remaining_images, gcp)\n",
    "                reconstructions.append(reconstruction)\n",
    "                reconstructions = sorted(reconstructions,\n",
    "                                         key=lambda x: -len(x.shots))\n",
    "                data.save_reconstruction(reconstructions)\n",
    "\n",
    "    for k, r in enumerate(reconstructions):\n",
    "        logger.info(\"Reconstruction {}: {} images, {} points\".format(\n",
    "            k, len(r.shots), len(r.points)))\n",
    "    logger.info(\"{} partial reconstructions in total.\".format(\n",
    "        len(reconstructions)))\n",
    "    chrono.lap('compute_reconstructions')\n",
    "    report['wall_times'] = dict(chrono.lap_times())\n",
    "    report['not_reconstructed_images'] = list(remaining_images)\n",
    "    return report\n",
    "# reconstruction.incremental_reconstruction = incremental_reconstruction\n",
    "incremental_reconstruction(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = data.load_reconstruction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (array([[  1.00000000e+00,   1.55548023e-18,  -6.93474255e-18],\n",
       "         [  1.55548023e-18,   1.00000000e+00,  -2.94817128e-18],\n",
       "         [ -6.93474255e-18,  -2.94817128e-18,   1.00000000e+00]]),\n",
       "  array([ -4.44089210e-16,   0.00000000e+00,   0.00000000e+00])),\n",
       " 2: (array([[ 0.07449526, -0.99720615, -0.00550825],\n",
       "         [-0.13674328, -0.00474353, -0.99059516],\n",
       "         [ 0.98780146,  0.07454786, -0.13671461]]),\n",
       "  array([-0.36075196,  0.49436254,  4.07025784])),\n",
       " 3: (array([[ 0.99988707,  0.01473158,  0.00297153],\n",
       "         [-0.0146786 ,  0.99974561, -0.01712479],\n",
       "         [-0.00322305,  0.01707923,  0.99984894]]),\n",
       "  array([ 0.01138758,  0.45821072, -0.17173804])),\n",
       " 4: (array([[ 0.04731907,  0.99880432, -0.01228126],\n",
       "         [ 0.11376182,  0.0068263 ,  0.9934846 ],\n",
       "         [ 0.99238055, -0.0484079 , -0.11330279]]),\n",
       "  array([ 0.48957148, -0.43405104,  4.02746834])),\n",
       " 5: (array([[  9.95930072e-01,   1.97614650e-02,  -8.79362095e-02],\n",
       "         [ -1.98910448e-02,   9.99801975e-01,  -5.97455404e-04],\n",
       "         [  8.79069893e-02,   2.34416689e-03,   9.96125929e-01]]),\n",
       "  array([ 4.86487477, -0.20926538, -0.2700696 ])),\n",
       " 6: (array([[ 0.08730827,  0.9961405 , -0.00902073],\n",
       "         [-0.05875864, -0.00388998, -0.99826464],\n",
       "         [-0.99444693,  0.0876868 ,  0.05819224]]),\n",
       "  array([ 0.78605285,  0.46929677, -0.77358605]))}"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformations = { im : (x.pose.get_rotation_matrix(), x.pose.translation) for r in rs for im, x in r.shots.items() }\n",
    "R1, t1 = transformations[1]\n",
    "trans1 = {} # camera 1 point of reference\n",
    "for im, (Ri, ti) in transformations.items():\n",
    "    R = np.dot(Ri, R1.T)\n",
    "    trans1[im] = (R, ti - np.dot(R, t1))\n",
    "trans1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rts_test = [transformations[1], transformations[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dict = frames[('soda_front', 800)]\n",
    "fs = [f_dict[cam] for cam in Cam_ids]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
